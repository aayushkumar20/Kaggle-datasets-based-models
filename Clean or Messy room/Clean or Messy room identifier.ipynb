{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-19T07:49:05.901316Z","iopub.execute_input":"2022-01-19T07:49:05.901719Z","iopub.status.idle":"2022-01-19T07:49:06.076286Z","shell.execute_reply.started":"2022-01-19T07:49:05.901609Z","shell.execute_reply":"2022-01-19T07:49:06.075349Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport cv2 \nfrom random import shuffle \nfrom tqdm import tqdm \nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"../input/messy-vs-clean-room\"))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:50:38.532696Z","iopub.execute_input":"2022-01-19T07:50:38.533008Z","iopub.status.idle":"2022-01-19T07:50:39.872330Z","shell.execute_reply.started":"2022-01-19T07:50:38.532969Z","shell.execute_reply":"2022-01-19T07:50:39.871252Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Explanation.\n\nIn this cell I have imported all the required modules and also accessed the directory of the given dataset.","metadata":{}},{"cell_type":"code","source":"train_messy = \"/kaggle/input/messy-vs-clean-room/images/images/train/messy/\"\ntrain_clean= \"/kaggle/input/messy-vs-clean-room/images/images/train/clean/\"\ntest_messy= \"/kaggle/input/messy-vs-clean-room/images/images/val/messy/\"\ntest_clean= \"/kaggle/input/messy-vs-clean-room/images/images/val/clean/\"\nimage_size = 128","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:50:43.441050Z","iopub.execute_input":"2022-01-19T07:50:43.441342Z","iopub.status.idle":"2022-01-19T07:50:43.446187Z","shell.execute_reply.started":"2022-01-19T07:50:43.441313Z","shell.execute_reply":"2022-01-19T07:50:43.445337Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"for picture in tqdm(os.listdir(train_messy)):\n    path=os.path.join(train_messy,picture)\n    img=cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n    img=cv2.resize(img,(image_size,image_size)).flatten()\n    np_img=np.asarray(img)\n    \nfor picture2 in tqdm(os.listdir(train_clean)):\n    path=os.path.join(train_clean,picture2)\n    pic=cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n    pic=cv2.resize(img,(image_size,image_size)).flatten()\n    np_pic=np.asarray(pic)\n\n\nplt.figure(figsize=(10,10))\nplt.subplot(1, 2, 1)\nplt.imshow(np_img.reshape(image_size, image_size))\nplt.axis('on')\nplt.subplot(1, 2, 2)\nplt.imshow(np_pic.reshape(image_size, image_size))\nplt.axis('on')","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:50:49.211870Z","iopub.execute_input":"2022-01-19T07:50:49.212635Z","iopub.status.idle":"2022-01-19T07:50:51.559565Z","shell.execute_reply.started":"2022-01-19T07:50:49.212587Z","shell.execute_reply":"2022-01-19T07:50:51.558600Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Explanation.\n\n- The code starts by importing the necessary packages.\n- Next, it creates a list of all the pictures in the train_messy directory and then loops through them to find one with a picture that is similar to what we are looking for.\n- The code then uses cv2.imread() to read in the image from disk and resize it so that it will fit on our screen (image_size, image_size).\n- It flattens out this array into np_img which is an numpy array of shape (image_size, image_size) .\n- Next, we create a list of all the pictures in train_clean and loop through them as well.\n- We use cv2.imread() again but this time we pass in \"gray\" instead of \"RGB\".\n- This tells cv2 not to convert colors from RGB into gray before reading them in from disk like normal images would be read into memory using cv2's imread function.\n- After reading these images back into memory they are resized so that they will also fit on our screen (image size, image size).\n- They are flattened out into np pic which is an numpy array of shape (image size, image size) .\nâ€“\n- The code attempts to show the difference between the input and output of a computer vision algorithm.\n\n- The code shows an image of a picture on top, followed by the corresponding \"clean\" version of that same picture.","metadata":{}},{"cell_type":"code","source":"def train_data():\n    train_data_messy = [] \n    train_data_clean=[]\n    for image in tqdm(os.listdir(train_messy)): \n        path = os.path.join(train_messy, image)\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n        img = cv2.resize(img, (image_size, image_size))\n        train_data_messy.append(img) \n    for picture in tqdm(os.listdir(train_clean)): \n        path = os.path.join(train_clean, picture)\n        pic = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n        pic = cv2.resize(pic, (image_size, image_size))\n        train_data_clean.append(pic) \n    \n    train_data= np.concatenate((np.asarray(train_data_messy),np.asarray(train_data_clean)),axis=0)\n    return train_data","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:00.596070Z","iopub.execute_input":"2022-01-19T07:51:00.596362Z","iopub.status.idle":"2022-01-19T07:51:00.603768Z","shell.execute_reply.started":"2022-01-19T07:51:00.596333Z","shell.execute_reply":"2022-01-19T07:51:00.602449Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Explanation.\n\n- The code starts by declaring a variable called train_data.\n- This is the list of images that will be used for training and testing.\n- The code then declares two variables, train_data_messy and train_data_clean, which are lists of images that have been preprocessed in different ways.\n- The first line creates an empty list called train_data.\n- Then the next line starts iterating through all the files in tqdm(os.listdir(train_messy)) to create a new image every time it's done with one file from tqdm(os.listdir(train_messy)).\n- The path variable is set to os.path.join(train_messy, image) so it can access each individual file from tqdm(os.listdir).\n- Next, cv2 is imported as well as some other libraries needed for processing images like cv2 import numpy as np and cv2 import matplotlib as plt .\n- Next up we use os to get information about where our data directory is located on our computer using os path = os .path .join (train _ mess y ,image) so we know what directory contains all the pictures we're\n- The code is a function that takes in an array of images and returns the corresponding train_data.\n\n- The first line creates a list of all the files in the directory called train_messy.\n- The second line creates a list of all the files in the directory called train_clean.\n- The third line takes each file from train_messy and converts it to grayscale, then resize it so that its size is equal to image size, which is 640x480 pixels for this example.","metadata":{}},{"cell_type":"code","source":"def test_data():\n    test_data_messy = [] \n    test_data_clean=[]\n    for image in tqdm(os.listdir(test_messy)): \n        path = os.path.join(test_messy, image)\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n        img = cv2.resize(img, (image_size, image_size))\n        test_data_messy.append(img) \n    for picture in tqdm(os.listdir(test_clean)): \n        path = os.path.join(test_clean, picture)\n        pic = cv2.imread(path, cv2.IMREAD_GRAYSCALE) \n        pic = cv2.resize(pic, (image_size, image_size))\n        test_data_clean.append(pic) \n    \n    test_data= np.concatenate((np.asarray(test_data_messy),np.asarray(test_data_clean)),axis=0) \n    return test_data","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:06.480008Z","iopub.execute_input":"2022-01-19T07:51:06.480301Z","iopub.status.idle":"2022-01-19T07:51:06.487813Z","shell.execute_reply.started":"2022-01-19T07:51:06.480268Z","shell.execute_reply":"2022-01-19T07:51:06.486896Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Explanation.\n\n- The code starts by declaring a variable called test_data.\n- This is the list of data that will be analyzed in this program.\n- The next line declares an empty list called test_data_clean, which will hold the cleaned up version of the original data.\n- Next, it iterates through all of the files in the directory where our image was found and creates a new list for each one with its path as its first element and then stores it into test_data.\n- It then goes on to create another empty list called test_data_messy, which holds all of the images that are not yet cleaned up.\n- After creating these two lists, it concatenates them together using np.concatenate().\n- The last line returns what we have created so far:\n- The code is used to create a list of images that are in the test_messy directory and then creates a list of images from the test_clean directory.\n- The code then concatenates both lists together into one list with the first axis being 0.","metadata":{}},{"cell_type":"code","source":"train_data=train_data()\ntest_data=test_data()","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:10.051014Z","iopub.execute_input":"2022-01-19T07:51:10.051833Z","iopub.status.idle":"2022-01-19T07:51:11.024002Z","shell.execute_reply.started":"2022-01-19T07:51:10.051788Z","shell.execute_reply":"2022-01-19T07:51:11.023000Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"x_data=np.concatenate((train_data,test_data),axis=0)\nx_data=(x_data-np.min(x_data))/(np.max(x_data)-np.min(x_data))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:13.958901Z","iopub.execute_input":"2022-01-19T07:51:13.959769Z","iopub.status.idle":"2022-01-19T07:51:13.994934Z","shell.execute_reply.started":"2022-01-19T07:51:13.959721Z","shell.execute_reply":"2022-01-19T07:51:13.994174Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Explanation.\n\n- The code starts by creating a list of the data points in the training set.\n- Then, it creates a new list with all the data points in the test set.\n- Next, it subtracts out any values that are less than or equal to 0 and divides by max(x_data) - min(x_data).\n- The result is then sorted into ascending order.\n- The code starts by creating two lists: one for each dataset (train and test).\n- It then subtracts out any values that are less than or equal to 0 and divides by max(x_data) - min(x_data).\n- The result is then sorted into ascending order.\n- The code will take the data from both the train and test sets, concatenate them together, then find the minimum value in that concatenated list.","metadata":{}},{"cell_type":"code","source":"z1=np.zeros(96)\no1=np.ones(96)\nY_train=np.concatenate((o1,z1),axis=0)\nz=np.zeros(10)\no=np.ones(10)\nY_test=np.concatenate((o,z),axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:18.633197Z","iopub.execute_input":"2022-01-19T07:51:18.633457Z","iopub.status.idle":"2022-01-19T07:51:18.638596Z","shell.execute_reply.started":"2022-01-19T07:51:18.633430Z","shell.execute_reply":"2022-01-19T07:51:18.637684Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"y_data=np.concatenate((Y_train,Y_test),axis=0).reshape(x_data.shape[0],1)\n\n#To print the values of X and Y axis.\n\nprint(x_data.shape)\nprint(y_data.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:21.927114Z","iopub.execute_input":"2022-01-19T07:51:21.927987Z","iopub.status.idle":"2022-01-19T07:51:21.933210Z","shell.execute_reply.started":"2022-01-19T07:51:21.927932Z","shell.execute_reply":"2022-01-19T07:51:21.932215Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Explanation.\n\n- The code starts by importing the numpy library.\n- Then it creates a variable called X_train and Y_train which are two matrices of size 100x100.\n- The code then reshapes the matrix to have one row and one column with shape (1,100).\n- The next line is where the data is split into training set and test set.\n- This is done by creating an array of size 1x2 which contains 0 for both rows and columns in order to create a new matrix that has only one row and one column with shape (1,2).\n- Next, this new matrix is concatenated with another array containing all values from Y_train on axis=0 so that they become a single vector.\n- The result of this operation will be stored in y_data .\n- The code will create a matrix of size (X_data.shape[0],1) and reshape it into the shape of (x_data.shape[0],1).","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=42)\nnumber_of_train = x_train.shape[0]\nnumber_of_test = x_test.shape[0]","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:25.723942Z","iopub.execute_input":"2022-01-19T07:51:25.724632Z","iopub.status.idle":"2022-01-19T07:51:25.755239Z","shell.execute_reply.started":"2022-01-19T07:51:25.724590Z","shell.execute_reply":"2022-01-19T07:51:25.754102Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Explanation.\n\n- The code splits the data into two sets, x_train and x_test.\n- The code then randomly assigns a percentage of 0.15 to each set so that it is equally likely for any given row in the dataset to be selected as either an example in the training or test set.\n- The shape of both sets are [n_train, n_test] where n is the number of rows in each set.\n- The code splits the data into two sets, x_train and x_test.\n- The test set is then randomly split into 15% of the total number of training examples.\n- The code above also computes the size of each training and test set in order to compute the size of each random subset.","metadata":{}},{"cell_type":"code","source":"x_train_flat = x_train.reshape(number_of_train,x_train.shape[1]*x_train.shape[2])\nx_test_flat = x_test .reshape(number_of_test,x_test.shape[1]*x_test.shape[2])\nprint(\"X train:\",x_train_flat.shape)\nprint(\"X test:\",x_test_flat.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:29.518686Z","iopub.execute_input":"2022-01-19T07:51:29.518971Z","iopub.status.idle":"2022-01-19T07:51:29.526131Z","shell.execute_reply.started":"2022-01-19T07:51:29.518931Z","shell.execute_reply":"2022-01-19T07:51:29.524755Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Explanation.\n\n- The code is trying to reshape the data from a matrix into a vector.\n- The code is doing this by taking the number of rows and columns in each dimension, multiplying them together, then dividing that result by the number of dimensions.\n- The first line creates an empty list called x_train_flat.\n- This will be used as a placeholder for all training data before it is reshaped into vectors.\n- The next line takes the training data and converts it into a matrix using NumPy's array function:\n- x_train = np.array(data) # convert to numpy array\n- print(\"X train:\",x_train) # print out shape of X train\n- print(\"X test:\",x_test) # print out shape of X test\n- This code prints out two things about each column in both arrays (the training and testing).\n- One thing printed is how many elements are in that column (number), followed by what type of object those elements are (shape).\n- The code will reshape the data into a matrix of shape (number_of_train, number_of_test) and then flatten it.","metadata":{}},{"cell_type":"code","source":"x_train = x_train_flat.T\nx_test = x_test_flat.T\ny_test = y_test.T\ny_train = y_train.T","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:34.098055Z","iopub.execute_input":"2022-01-19T07:51:34.098950Z","iopub.status.idle":"2022-01-19T07:51:34.104370Z","shell.execute_reply.started":"2022-01-19T07:51:34.098898Z","shell.execute_reply":"2022-01-19T07:51:34.103453Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Explanation.\n\nThe code attempts to create a training set of data and a test set of data.","metadata":{}},{"cell_type":"code","source":"# Algorithm for identifying the clean and messy room.\n# It'll also print the accurace of test and training.\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b\n\ndef sigmoid(z):\n    y_head = 1/(1+np.exp(-z))\n    return y_head\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients\n\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(number_of_iterarion):\n        \n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 100 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n\ndef predict(w,b,x_test):\n    \n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n\n    dimension =  x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    print(\"Test Accuracy: {} %\".format(round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,2)))\n    print(\"Train Accuracy: {} %\".format(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,2)))","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:38.147425Z","iopub.execute_input":"2022-01-19T07:51:38.148056Z","iopub.status.idle":"2022-01-19T07:51:38.164861Z","shell.execute_reply.started":"2022-01-19T07:51:38.148003Z","shell.execute_reply":"2022-01-19T07:51:38.163755Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Explanation.\n- The code starts by initializing the weights and bias of a neural network.\n- The sigmoid function is used to calculate the output from the input.\n- Forward-backward propagation is then used to calculate cost and gradients for updating weights and biases.\n- Finally, logistic regression is performed on test data in order to predict values for future data points.\n- The code starts by initializing the weights and bias of a neural network with w = np.full((dimension,1),0.01) b = 0.0 \treturn w, b .\n- Then forward-backward propagation is used to calculate cost and gradients for updating these parameters using x_train , y_train , x_test , y_test .\n- Finally, logistic regression is performed on test data in order to predict values for future data points using Y_prediction = np.zeros((1,x_test shape[1])) return Y_prediction- The code is a logistic regression model.\n- The code above first initializes the weights and bias of the model.\n- Then, it performs forward and backward propagation to calculate the cost function.\n- Finally, it prints out the result of each iteration after 100 iterations are completed.","metadata":{}},{"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 1500)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:44.559834Z","iopub.execute_input":"2022-01-19T07:51:44.560485Z","iopub.status.idle":"2022-01-19T07:51:47.122348Z","shell.execute_reply.started":"2022-01-19T07:51:44.560451Z","shell.execute_reply":"2022-01-19T07:51:47.121446Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"grid={\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]},\nlogistic_regression=LogisticRegression(random_state=20)\nlog_reg_cv=GridSearchCV(logistic_regression,grid,cv=10)\nlog_reg_cv.fit(x_train.T,y_train.T)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:51:50.416879Z","iopub.execute_input":"2022-01-19T07:51:50.417163Z","iopub.status.idle":"2022-01-19T07:52:30.616313Z","shell.execute_reply.started":"2022-01-19T07:51:50.417134Z","shell.execute_reply":"2022-01-19T07:52:30.615497Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Explanation.\n\n- The code starts by importing the necessary libraries.\n- Next, a grid is created with 7 columns and 3 rows.\n- The first column has a penalty of l1 for negative values and l2 for positive values.\n- The second column has a penalty of -l1 for negative values and +l2 for positive values.\n- Next, an instance of LogisticRegression is created with random_state=42 as the seed value to initialize it from.\n- A GridSearchCV object is then created using logistic regression as the search algorithm and 10 as the CV parameter (confusion matrix).\n- Finally, this object's fit method is called on x_train which returns predictions y_train .\n- The code is to fit a logistic regression model on the training data.\n- The code above takes in x_train and y_train as input, and then runs a grid search with 10-fold cross validation.","metadata":{}},{"cell_type":"code","source":"print(\"best hyperparameters: \", log_reg_cv.best_params_)\nprint(\"accuracy: \", log_reg_cv.best_score_)","metadata":{"execution":{"iopub.status.busy":"2022-01-19T07:53:16.009522Z","iopub.execute_input":"2022-01-19T07:53:16.009872Z","iopub.status.idle":"2022-01-19T07:53:16.016731Z","shell.execute_reply.started":"2022-01-19T07:53:16.009836Z","shell.execute_reply":"2022-01-19T07:53:16.015377Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Explanation.\n\nThe code will print out the best hyperparameters and accuracy of logistic regression.","metadata":{}}]}